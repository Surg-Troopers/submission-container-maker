"""
The following is a simple example algorithm.

It is meant to run within a container.

To run the container locally, you can call the following bash script:

  ./do_test_run.sh

This will start the inference and reads from ./test/input and writes to ./test/output

To save the container and prep it for upload to Grand-Challenge.org you can call:

  ./do_save.sh

Any container that shows the same behaviour will do, this is purely an example of how one COULD do it.

Reference the documentation to get details on the runtime environment on the platform:
https://grand-challenge.org/documentation/runtime-environment/

Happy programming!
"""

from pathlib import Path
import json
import torch, ultralytics
from ultralytics import YOLO
import output_format
import frame_extract

INPUT_PATH = Path("/input")
OUTPUT_PATH = Path("/output")
RESOURCE_PATH = Path("resources")


def run():
    # The key is a tuple of the slugs of the input sockets
    interface_key = get_interface_key()

    # Lookup the handler for this particular set of sockets (i.e. the interface)
    handler = {
        ("endoscopic-robotic-surgery-video",): interf0_handler,
    }[interface_key]

    # Call the handler
    return handler()


def interf0_handler():
    # Read the input
    frame_extract.extract_frames("/input/endoscopic-robotic-surgery-video.mp4","images" , 1)

    # Process the inputs: any way you'd like
    _show_torch_cuda_info()

    # Some additional resources might be required, include these in one of two ways.

    # Option 1: part of the Docker-container image: resources/
    resource_dir = Path("/opt/app/resources")
    with open(resource_dir / "some_resource.txt", "r") as f:
        print(f.read())

    # # Option 2: upload them as a separate tarball to Grand Challenge (go to your Algorithm > Models). The resources in the tarball will be extracted to `model_dir` at runtime.
    # model_dir = Path("/opt/ml/model")
    # with open(
    #     model_dir / "a_tarball_subdirectory" / "some_tarball_resource.txt", "r"
    # ) as f:
    #     print(f.read())

    # For now, let us make bogus predictions
    # list of tools must follow the list below:
            # self.tool_list = ["needle_driver",
            #               "monopolar_curved_scissor",
            #               "force_bipolar",
            #               "clip_applier",
            #               "tip_up_fenestrated_grasper",
            #               "cadiere_forceps",
            #               "bipolar_forceps",
            #               "vessel_sealer",
            #               "suction_irrigator",
            #               "bipolar_dissector",
            #               "prograsp_forceps",
            #               "stapler",
            #               "permanent_cautery_hook_spatula",
            #               "grasping_retractor"]


    print("PyTorch:", torch.__version__)

    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    print("Device available:", DEVICE)

    print("Ultralytics:", ultralytics.__version__)

    model = YOLO("model/a_tarball_subdirectory/best.pt")
    # video_path = "/mnt/d/SurgVU 25/surgvu24_videos_only/surgvu24/case_001/case_001_video_part_001.mp4""
    pred = model.predict(
        source="images",
        imgsz=512,
        max_det = 4,
        conf=0.25, # confidence threshold set to 0.25 temp
    )

    output_surgical_tools = output_format.output_format_as_json(pred, OUTPUT_PATH)


    # Save your output
    # write_json_file(
    #     location=OUTPUT_PATH / "output.json", content=output_surgical_tools
    # )
    print('json file generated by the submission container')

    return 0


def get_interface_key():
    # The inputs.json is a system generated file that contains information about
    # the inputs that interface with the algorithm
    inputs = load_json_file(
        location=INPUT_PATH / "inputs.json",
    )
    print('inputs - ', inputs)
    socket_slugs = [sv["interface"]["slug"] for sv in inputs]
    print('socket slugs', socket_slugs)
    return tuple(sorted(socket_slugs))


def load_json_file(*, location):
    # Reads a json file
    with open(location, "r") as f:
        data = f.read()  # decode bytes into string
        return json.loads(data)


    
def write_json_file(*, location, content):
    # Writes a json file
    with open(location, "w") as f:
        f.write(json.dumps(content, indent=4))


# Note to the developer:
#   the following function is very generic and should likely
#   be adopted to something more specific for your algorithm/challenge
def load_file(location):
    with open(location) as f:
        return f.read()



def _show_torch_cuda_info():
    import torch

    print("=+=" * 10)
    print("Collecting Torch CUDA information")
    print(f"Torch CUDA is available: {(available := torch.cuda.is_available())}")
    if available:
        print(f"\tnumber of devices: {torch.cuda.device_count()}")
        print(f"\tcurrent device: { (current_device := torch.cuda.current_device())}")
        print(f"\tproperties: {torch.cuda.get_device_properties(current_device)}")
    print("=+=" * 10)


if __name__ == "__main__":
    raise SystemExit(run())
